# -*- coding: utf-8 -*-
"""StackOverflowScraper.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bx4eHPJnP0QQpmcIebXTB3JGiWoTg3vr
"""

import requests
from bs4 import BeautifulSoup as bs
import urllib
import pandas as pd

def get_top_questions(url):
    # Using requests module for downloading webpage content
    response = requests.get(url)
    # Parsing html data using BeautifulSoup
    soup = bs(response.content, 'html.parser')
    body = soup.find('body')
   # print(body)
    #find all questions
    questions= body.find_all('div', class_='question-summary')
    #print(questions[0])
    df = pd.DataFrame(columns=['question', 'summary','tags', 'votes','accepted','answer'])
    #extract the data from questions
    i=0
    for quest in questions:
      urlQuest= quest.select("h3 a.question-hyperlink")[0]['href']
      #print(urlQuest)
      summary=quest.select("h3 a.question-hyperlink")[0].text
      #parse_question(urlQuest,summary)
      df = df.append(parse_question("https://stackoverflow.com/"+urlQuest,summary))
      
    return df

import requests
from bs4 import BeautifulSoup as bs
import urllib
import pandas as pd

# Using requests module for downloading webpage content
response = requests.get("https://stackoverflow.com/questions/31674557/how-to-append-rows-in-a-pandas-dataframe-in-a-for-loop")
# Parsing html data using BeautifulSoup
soup = bs(response.content, 'html.parser')
body = soup.find('body')
body
# print(body)
#id=comments-link-46397882
#find all questions
#CSS selector -replace nth-child with nth-of-type
#answer=body.select('#answer-31675177 > div:nth-of-type(1) > div:nth-of-type(2) > div:nth-of-type(1)')
#answer= body.find('a')
answer
from lxml import etree
from lxml import html

tree = html.fromstring(response.content)
#This will create a list of buyers:
buyers = tree.xpath('/html/body/div[4]/div[2]/div/div[1]/div[3]/div[3]/div[2]/div/div[3]')
buyers

import requests
from bs4 import BeautifulSoup as bs
import urllib
import pandas as pd
import numpy as np
def parse_question(url,summary):
    #response = requests.get("https://stackoverflow.com/questions/31674557/how-to-append-rows-in-a-pandas-dataframe-in-a-for-loop")
    # Using requests module for downloading webpage content
    response = requests.get(url)
    #print("https://stackoverflow.com/"+url)
    # Parsing html data using BeautifulSoup
    soup = bs(response.content, 'html.parser')
    body1 = soup.find('body')

    #create df
    dftemp = pd.DataFrame(columns=['question', 'summary','tags', 'votes','accepted','answer'])

    #getting tags
    tags=' '.join(e.text for e in list(set(body1.find_all('a', class_='post-tag'))))
    #print("tags************")
    #print(tags)
    #getting question
    try:
      question=body1.find('a',class_='question-hyperlink').text
    except:
      print(url+" is not valid URL")
      return dftemp
   # print("questions****************")
    #print(question)
    #getting answer
    answers = body1.find_all('div', class_='answer')

    counter=0;
    for answer in answers :
      #print(answer)
      codeSample=' \n '.join(e.text for e in answer.select('code'))
      voteCount=answer.find('div', class_='js-vote-count grid--cell fc-black-500 fs-title grid fd-column ai-center').text
      #print("codeSample******************")
      #print(codeSample)
     # print("voteCount******************")
     # print(voteCount)
      # Putting all of them together
      accepted = 0
      #***********************TODO***************************
      #fix
      if(answer.find('div', class_='accepted-answer')):
        accepted = 1
      dftemp.loc[counter] = np.array([question, summary, tags, voteCount, accepted, codeSample])
      counter=counter+1
    #print(dftemp)
    return dftemp

data=get_top_questions("https://stackoverflow.com/questions/tagged/python?tab=bounties&page=1&pagesize=30")
data=data.reset_index(drop=True)

from google.colab import files
def get_data_by_topic(wanted_pages,tags,pagesize,tab):
  #https://stackoverflow.com/questions/tagged/python%2bmachine-learning?tab=active&page=2&pagesize=30
  tagsStr = '%2'.join(str(e) for e in tags)
  data = pd.DataFrame(columns=['question', 'summary','tags', 'votes','accepted','answer'])
  for i in range(wanted_pages):
    url="https://stackoverflow.com/questions/tagged/"+tagsStr+"?tab="+tab+"&page="+str(i)+"&pagesize="+str(pagesize)
    data.append(get_top_questions(url))
  data=data.reset_index(drop=True)
  #Exctract to exel
  data.to_excel('result.xlsx') 
  files.download('result.xlsx')

get_data_by_topic(2,['python'],15,'Bounties')